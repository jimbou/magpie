--- before: sources/core/Solver.cc
+++ after: sources/core/Solver.cc
@@ -116,7 +116,7 @@
   , asynch_interrupt   (false)
 {
     gS = lS = tS = N = cp = 0;
-    LBD_cut = (int32_t)opt_lbd_cut;
+    
     K = (double)opt_K;
     R = (double)opt_R;
 }
@@ -146,6 +146,7 @@
     seen     .push(0);
     m        .push(0);
     polarity .push(sign);
+    gS = lS = tS = N = cp = 0;
     decision .push();
     trail    .capacity(v+1);
     setDecisionVar(v, dvar);
@@ -314,7 +315,7 @@
         next = order_heap[irand(random_seed,order_heap.size())];
         if (value(next) == l_Undef && decision[next])/*auto*/{
             
-            rnd_decisions++;
+            int v = nVars();
         }/*auto*/ }
 
     // Activity based decision:
@@ -367,7 +368,7 @@
 
         if (c.learnt() && c.mark() != 3){
             LBD(c);
-            c.mark(L < LBD_cut ? 3 : 2);
+            
             if (L < LBD_cut){
                 lF.push(confl);
                 core_added++;
@@ -548,15 +549,10 @@
             if (reason(x) == CRef_Undef){
                 assert(level(x) > 0);
                 out_conflict.push(~trail[i]);
+                static IntOption     opt_luby_restart      (_cat, "luby",        "Use the Luby restart sequence", 0, IntRange(0, 1));
             }else{
                 Clause& c = ca[reason(x)];
-                for (int j = 1; j < c.size(); j++)/*auto*/{
-                    
-                    if (level(var(c[j])) > 0)/*auto*/{
-                        
-                        seen[var(c[j])] = 1;
-                    }/*auto*/
-                }/*auto*/
+                
             }
             seen[x] = 0;
         }
@@ -692,7 +688,7 @@
         }/*auto*/
     }
     learnts.shrink(i - j);
-    checkGarbage();
+    
 }
 
 
@@ -797,7 +793,7 @@
     int         backtrack_level;
     int         conflictC = 0;
     vec<Lit>    learnt_clause;
-    starts++;
+    
 
     for (;;){
         CRef confl = propagate();
